{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagizterzi/ImageCaptionGenerator/blob/main/%C4%B0mageCaption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAJJTAOyR2Zm"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c obss-intern-competition-2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrMAU4XMR_j4"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Specify the path to your zip file\n",
        "zip_file_path = '/content/obss-intern-competition-2025.zip' # Replace with the actual path\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extraction_dir = '/content/' # Replace with your desired directory name\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extraction_dir):\n",
        "    os.makedirs(extraction_dir)\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents into the specified directory\n",
        "    zip_ref.extractall(extraction_dir)\n",
        "\n",
        "print(f\"Zip file extracted to: {extraction_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJgwWQAFWoWq"
      },
      "source": [
        "**Image caption generator using transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzN3Jx6qXM7H"
      },
      "source": [
        "Before strarting download all the libaries necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jShbEYtfJwDd"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas.api.types\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "from typing import List\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA5fmUi-XT_V"
      },
      "source": [
        "# 1-) Download the BLIP model\n",
        "BLIP is a VLP(Vision-Language Pre training) framework created by Salesforce research it designed for image captioning and visual question answering.It generates natural language descriptions from input images using a transformer-based architecture.\n",
        "\n",
        "**Processor** turns the image into a tensor using feature extractor and tokenizes the text\n",
        "\n",
        "**Model** captions the image with creating relation between tokenized text and image tensor using Transformer based architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxd1-znLXxj6"
      },
      "outputs": [],
      "source": [
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YKW-ckgazZl"
      },
      "source": [
        "# 2-) Create a dataset class\n",
        "Dataset class is designed to load image data and corresponding captions for use in training or evaluating a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBRPMxTmatRe"
      },
      "outputs": [],
      "source": [
        "max_length = 128\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, csv_path, image_folder, processor, train=True):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.image_folder = image_folder\n",
        "        self.processor = processor\n",
        "        self.train = train\n",
        "# Method that returns the total number of samples in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "# Method for getting a single from the dataset at a given index\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        image_path = os.path.join(self.image_folder, str(item['image_id'])+'.jpg')\n",
        "        # Convert the image RGB format for input uniformity\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        if self.train:\n",
        "            caption = item['caption']\n",
        "            # Process the image and text using the processor\n",
        "            encoding = self.processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True,max_length=max_length)\n",
        "            encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "            encoding['labels'] = encoding['input_ids']\n",
        "        else:\n",
        "          # If not in training mode, process only the image\n",
        "            encoding = self.processor(images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True,max_length=max_length)\n",
        "            encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ6XJrltffs7"
      },
      "source": [
        "# 3-) Create a data loader\n",
        "Create data loader firstly using our dataset class then using the **DataLoader** from Pytorch\n",
        "\n",
        "Load seperate for testing and training so we can process both text and image or only image depending on our goal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnXd57Hkffj2"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(\"/content/train.csv\", \"/content/train/train\", processor, train=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
        "\n",
        "test_dataset = Dataset(\"/content/test.csv\", \"/content/test/test\", processor, train=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yme8RN5pmuqX"
      },
      "source": [
        "# 4-) Train the model\n",
        "During training, the model receives images and captions from the DataLoader.\n",
        "Images are converted to tensors using the processor and passed to the model.\n",
        "The predicted captions are compared to the ground truth to calculate the loss.\n",
        "Based on the loss, model weights are updated and the process repeats for each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq_HQ12Knz62"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 10\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    epoch_losses = []\n",
        "\n",
        "    # For GPU Optimization\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        # Move batch to device safely\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Forward + Backward\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    epoch_losses.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), f\"blip_epoch{epochs}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Loss chart for trained model"
      ],
      "metadata": {
        "id": "qhzg4iLjKwdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data\n",
        "epochs = list(range(1, 11))\n",
        "loss = [0.8526, 0.2746, 0.2040, 0.1496, 0.1063, 0.0729, 0.0502, 0.0355, 0.0269, 0.0225]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, loss, marker='o', linestyle='-', color='blue')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.xticks(epochs)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jxtFOobcHMlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the trained models weights and train the text decoder for better results"
      ],
      "metadata": {
        "id": "ZaSKxNK6Ndp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(\"blip_epoch10.pth\")\n",
        "model.load_state_dict(state_dict)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Stage 1: Fine-tune the text decoder\n",
        "for name, param in model.named_parameters():\n",
        "    if \"text_decoder\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-6)\n",
        "\n",
        "num_epochs = 2\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        images = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=images, input_ids=inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Decoder Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Stage 2: Fine-tune the last layers of the visual backbone (layer11, layer12)\n",
        "for name, param in model.named_parameters():\n",
        "    if \"vision_model.encoder.layers.10\" in name or \"vision_model.encoder.layers.11\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "learnable_params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "optimizer = torch.optim.AdamW(learnable_params,lr=1e-6)\n",
        "\n",
        "num_epochs_backbone = 3\n",
        "for epoch in range(num_epochs_backbone):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        images = batch['pixel_values'].to(device)\n",
        "        captions_input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "\n",
        "        outputs = model(pixel_values=images, input_ids=captions_input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Backbone Epoch {epoch+1}/{num_epochs_backbone}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save the fully fine-tuned model\n",
        "torch.save(model.state_dict(), \"blip_finetuned.pth\")"
      ],
      "metadata": {
        "id": "ROCsvtoXMZ1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"blip_finetuned.pth\"))\n",
        "# Free up memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Move model to device and enable all parameters for training\n",
        "model.to(device)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define optimizer for all parameters\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "# Final full model training\n",
        "final_epochs = 5\n",
        "model.train()\n",
        "\n",
        "for epoch in range(final_epochs):\n",
        "    total_loss = 0.0\n",
        "    each_loss = []\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Final Training Epoch {epoch+1}/{final_epochs}\"):\n",
        "        # Move inputs to device\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        images = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values=images, input_ids=inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    each_loss.append(avg_loss)\n",
        "    print(f\"Final Training Epoch {epoch+1}/{final_epochs}, Loss: {avg_loss:.4f}\")\n",
        "# Save final model after all fine-tuning steps\n",
        "torch.save(model.state_dict(), \"blip_final.pth\")"
      ],
      "metadata": {
        "id": "cl2reiqtCYQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "epochs = list(range(1, 6))\n",
        "loss = [0.0035,0.0027,0.0022,0.0019,0.0017]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, loss, marker='o', linestyle='-', color='blue')\n",
        "plt.title('Final Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.xticks(epochs)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jjwEa52RFPuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwLxSIg-wK2G"
      },
      "outputs": [],
      "source": [
        "# Generate captions and create an empty list to store those captions\n",
        "\n",
        "# Load the trained model state dictionary\n",
        "model.load_state_dict(torch.load(\"blip_final.pth\"))\n",
        "\n",
        "model.eval()\n",
        "predicted_captions = []\n",
        "\n",
        "\n",
        "# Disable gradient calculation to save memory\n",
        "with torch.no_grad():\n",
        "  # Loop over the test data\n",
        "    for batch in test_loader:\n",
        "      # Extract pixel values and generate captions after that decode the text into normal\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        output_ids = model.generate(pixel_values=pixel_values, max_length=128)\n",
        "        captions = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "        predicted_captions.extend(captions)\n",
        "\n",
        "# Save the captions into a .csv file\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "test_df[\"caption\"] = predicted_captions\n",
        "test_df.to_csv(\"results.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
